"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"YXJ8MV63","preprint","2019","Benjamin, Misha; Gagnon, Paul; Rostamzadeh, Negar; Pal, Chris; Bengio, Yoshua; Shee, Alex","Towards Standardization of Data Licenses: The Montreal Data License","","","","10.48550/arXiv.1903.12262","http://arxiv.org/abs/1903.12262","This paper provides a taxonomy for the licensing of data in the fields of artificial intelligence and machine learning. The paper's goal is to build towards a common framework for data licensing akin to the licensing of open source software. Increased transparency and resolving conceptual ambiguities in existing licensing language are two noted benefits of the approach proposed in the paper. In parallel, such benefits may help foster fairer and more efficient markets for data through bringing about clearer tools and concepts that better define how data can be used in the fields of AI and ML. The paper's approach is summarized in a new family of data license language - \textit{the Montreal Data License (MDL)}. Alongside this new license, the authors and their collaborators have developed a web-based tool to generate license language espousing the taxonomies articulated in this paper.","2019-03-20","2025-01-03 15:03:34","2025-01-03 15:03:34","2024-09-20 13:52:15","","","","","","","Towards Standardization of Data Licenses","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1903.12262 [cs, stat]","","","notion://www.notion.so/Towards-Standardization-of-Data-Licenses-The-Montreal-Data-License-151ed28d384981058e4ecfcd4fecc21b","notion","Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1903.12262","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAZBFGAL","journalArticle","2019","Arnold, M.; Bellamy, R. K. E.; Hind, M.; Houde, S.; Mehta, S.; Mojsilović, A.; Nair, R.; Ramamurthy, K. Natesan; Olteanu, A.; Piorkowski, D.; Reimer, D.; Richards, J.; Tsay, J.; Varshney, K. R.","FactSheets: Increasing trust in AI services through supplier's declarations of conformity","IBM Journal of Research and Development","","0018-8646","10.1147/JRD.2019.2942288","https://ieeexplore.ieee.org/abstract/document/8843893","Accuracy is an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety (which includes fairness and explainability), security, and provenance, are also critical elements to engender consumers’ trust in a service. Many industries use transparent, standardized, but often not legally required documents called supplier's declarations of conformity (SDoCs) to describe the lineage of a product along with the safety and performance testing it has undergone. SDoCs may be considered multidimensional fact sheets that capture and quantify various aspects of the product and its development to make it worthy of consumers’ trust. In this article, inspired by this practice, we propose FactSheets to help increase trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers. We suggest a comprehensive set of declaration items tailored to AI in the Appendix of this article.","2019-07","2025-01-03 15:03:34","2025-01-03 15:03:34","2024-08-05 14:35:58","6:1-6:13","","4/5","63","","","FactSheets","","","","","","","","","","","","IEEE Xplore","","Conference Name: IBM Journal of Research and Development","","","notion://www.notion.so/FactSheets-Increasing-trust-in-AI-services-through-supplier-s-declarations-of-conformity-151ed28d38498162a3adf73caedd6c83","notion","Artificial intelligence; Industries; Safety; Security; Software; Standards; Testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FV8ZH6I3","journalArticle","2018","Bender, Emily M.; Friedman, Batya","Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science","Transactions of the Association for Computational Linguistics","","2307-387X","10.1162/tacl_a_00041","https://doi.org/10.1162/tacl_a_00041","In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","2018-12-01","2025-01-03 15:03:34","2025-01-03 15:03:34","2024-08-04 20:10:21","587-604","","","6","","Transactions of the Association for Computational Linguistics","Data Statements for Natural Language Processing","","","","","","","","","","","","Silverchair","","","","","notion://www.notion.so/Data-Statements-for-Natural-Language-Processing-Toward-Mitigating-System-Bias-and-Enabling-Better-S-151ed28d3849816dad02de69ae13bf06","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SQJSXE6","conferencePaper","2022","Adkins, David; Alsallakh, Bilal; Cheema, Adeel; Kokhlikyan, Narine; McReynolds, Emily; Mishra, Pushkar; Procope, Chavez; Sawruk, Jeremy; Wang, Erin; Zvyagina, Polina","Method cards for prescriptive machine-learning transparency","Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI","978-1-4503-9275-4","","10.1145/3522664.3528600","https://dl.acm.org/doi/10.1145/3522664.3528600","Specialized documentation techniques have been developed to communicate key facts about machine-learning (ML) systems and the datasets and models they rely on. Techniques such as Datasheets, AI FactSheets, and Model Cards have taken a mainly descriptive approach, providing various details about the system components. While the above information is essential for product developers and external experts to assess whether the ML system meets their requirements, other stakeholders might find it less actionable. In particular, ML engineers need guidance on how to mitigate potential shortcomings in order to fix bugs or improve the system's performance. We propose a documentation artifact that aims to provide such guidance in a prescriptive way. Our proposal, called Method Cards, aims to increase the transparency and reproducibility of ML systems by allowing stakeholders to reproduce the models, understand the rationale behind their designs, and introduce adaptations in an informed way. We showcase our proposal with an example in small object detection, and demonstrate how Method Cards can communicate key considerations that help increase the transparency and reproducibility of the detection model. We further highlight avenues for improving the user experience of ML engineers based on Method Cards.","2022-10-17","2024-12-17 12:38:49","2025-02-13 15:25:59","2025-01-09","90–100","","","","","","","CAIN '22","","","","Association for Computing Machinery","New York, NY, USA","en","","","","","ACM Digital Library","","tex.ids= adkinsMethodCardsPrescriptive2022a, adkinsMethodCardsPrescriptive2022b","","/Users/pcuellar/Zotero/storage/SCTN4STU/Adkins et al. - 2022 - Method cards for prescriptive machine-learning transparency.pdf","","","Adaptation models; Developer Experience; Documentation; Machine learning; Machine learning algorithms; Method Cards; Object detection; Reproducibility of results; Transparency; User experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CAIN '22: 1st Conference on AI Engineering - Software Engineering for AI","","","","","","","","","","","","","","",""
"NZZIJP3P","conferencePaper","2024","Bhardwaj, Eshta; Gujral, Harshit; Wu, Siyi; Zogheib, Ciara; Maharaj, Tegan; Becker, Christoph","Machine learning data practices through a data curation lens: An evaluation framework","Proceedings of the 2024 ACM conference on fairness, accountability, and transparency","979-8-4007-0450-5","","10.1145/3630106.3658955","https://doi.org/10.1145/3630106.3658955","Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.","2024","2024-12-17 13:04:55","2025-02-13 15:25:59","2024-12-17 13:04:55","1055–1067","","","","","","Machine learning data practices through a data curation lens","FAccT '24","","","","Association for Computing Machinery","New York, NY, USA","en","","","","","DOI.org (Crossref)","","Citation Key: 10.1145/3630106.3658955 tex.ids= bhardwajMachineLearningData2024a, bhardwajMachineLearningData2024b, bhardwajMachineLearningData2024d numPages: 13 place: Rio de Janeiro, Brazil","","","notion://www.notion.so/Machine-learning-data-practices-through-a-data-curation-lens-An-evaluation-framework-151ed28d3849814c8e26f87adbc5bbfe","data practices; documentation; notion; machine learning; datasheets; dataset creation; datasets; evaluation; rubric","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","FAccT '24: The 2024 ACM Conference on Fairness, Accountability, and Transparency","","","","","","","","","","","","","","",""
"DJGCYHXT","conferencePaper","2022","Atli Tekgul, Buse Gul; Asokan, N.","On the effectiveness of dataset watermarking","Proceedings of the 2022 ACM on international workshop on security and privacy analytics","978-1-4503-9230-3","","10.1145/3510548.3519376","https://doi.org/10.1145/3510548.3519376","In a data-driven world, datasets constitute a significant economic value. Dataset owners who spend time and money to collect and curate the data are incentivized to ensure that their datasets are not used in ways that they did not authorize. When such misuse occurs, dataset owners need technical mechanisms for demonstrating their ownership of the dataset in question. Dataset watermarking provides one approach for ownership demonstration which can, in turn, deter unauthorized use. In this paper, we investigate a recently proposed data provenance method, radioactive data, to assess if it can be used to demonstrate ownership of (image) datasets used to train machine learning (ML) models. The original paper radioactive reported that radioactive data is effective in white-box settings. We show that while this is true for large datasets with many classes, it is not as effective for datasets where the number of classes is low (łeq 30) or the number of samples per class is low (łeq 500). We also show that, counter-intuitively, the black-box verification technique described in radioactive is effective for all datasets used in this paper, even when white-box verification in radioactive is not. Given this observation, we show that the confidence in white-box verification can be improved by using watermarked samples directly during the verification process. We also highlight the need to assess the robustness of radioactive data if it were to be used for ownership demonstration since it is an adversarial setting unlike provenance identification. Compared to dataset watermarking, ML model watermarking has been explored more extensively in recent literature. However, most of the state-of-the-art model watermarking techniques can be defeated via model extraction robustness. We show that radioactive data can effectively survive model extraction attacks, which raises the possibility that it can be used for ML model ownership verification robust against model extraction.","2022","2025-02-12 22:13:36","2025-02-12 22:13:36","","93–99","","","","","","","Iwspa '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","Citation Key: 10.1145/3510548.3519376 Number of pages: 7 Place: Baltimore, MD, USA","","/Users/pcuellar/Zotero/storage/CD34536Q/Atli Tekgul and Asokan - 2022 - On the effectiveness of dataset watermarking.pdf","","deep neural networks; ownership verification; watermarking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AJZEE29","conferencePaper","2022","Ashurst, Carolyn; Hine, Emmie; Sedille, Paul; Carlier, Alexis","AI ethics statements: Analysis and lessons learnt from NeurIPS broader impact statements","Proceedings of the 2022 ACM conference on fairness, accountability, and transparency","978-1-4503-9352-2","","10.1145/3531146.3533780","https://doi.org/10.1145/3531146.3533780","Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.","2022","2025-02-12 22:13:37","2025-02-12 22:13:37","","2047–2056","","","","","","","FAccT '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","Citation Key: 10.1145/3531146.3533780 Number of pages: 10 Place: Seoul, Republic of Korea","","/Users/pcuellar/Zotero/storage/WGUQJ4S5/Ashurst et al. - 2022 - AI ethics statements Analysis and lessons learnt from NeurIPS broader impact statements.pdf","","AI governance; broader impacts; conference policies; ethics statements; NeurIPS policies; research ethics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZJK6ZQN","conferencePaper","2023","Barman, Nabajeet; Reznik, Yuriy; Martini, Maria","Datasheet for subjective and objective quality assessment datasets","2023 15th international conference on quality of multimedia experience (QoMEX)","","","10.1109/QoMEX58391.2023.10178546","","Over the years, many subjective and objective quality assessment datasets have been created and made available to the research community. However, there is no standard process for documenting the various aspects of the dataset, such as details about the source sequences, number of test subjects, test methodology, encoding settings, etc. Such information is often of great importance to the users of the dataset as it can help them get a quick understanding of the motivation and scope of the dataset. Without such a template, it is left to each reader to collate the information from the relevant publication or website, which is a tedious and time-consuming process. In some cases, the absence of a template to guide the documentation process can result in an unintentional omission of some important information. This paper addresses this simple but significant gap by proposing a datasheet template for documenting various aspects of sub-jective and objective quality assessment datasets for multimedia data. The contributions presented in this work aim to simplify the documentation process for existing and new datasets and improve their reproducibility. The proposed datasheet template is available on GitHub1, along with a few sample datasheets of a few open-source audiovisual subjective and objective datasets.","2023-06","2025-02-12 22:14:09","2025-02-12 22:14:09","","111-114","","","","","","","","","","","","","","","","","","","","Citation Key: 10178546 ISSN: 2472-7814","","/Users/pcuellar/Zotero/storage/2IJ8GKMF/Barman et al. - 2023 - Datasheet for subjective and objective quality assessment datasets.pdf","","Documentation; Standards; Databases; Reproducibility of results; Datasets; Encoding; Multimedia; Objective Assessment; Open-Source; QoE; Quality assessment; Subjective Assessment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K54XAH72","conferencePaper","2022","Agarwal, Sharat; Muku, Sumanyu; Anand, Saket; Arora, Chetan","Does data repair lead to fair models? Curating contextually fair data to reduce model bias","2022 IEEE/CVF winter conference on applications of computer vision (WACV)","","","10.1109/WACV51458.2022.00395","","Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. However, co-occurrence bias in the training dataset may hamper a DNNmodel’s generalizabil- ity to unseen scenarios in the real world. For example, in COCO [26], many object categories have a much higher cooccurrence with men compared to women, which can bias a DNN’s prediction in favor of men. Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, which is fair in terms of the co-occurrence with various classes for a protected attribute. We introduce a data repair algorithm using the coefficient of variation( cv), which can curate fair and contextually balanced data for a protected class(es). This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective and can even be used in an active learning setting where the data labels are not present or being generated incrementally. We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model’s overall performance. Code: https://github.com/sumanyumuku98/contextual-bias","2022-01","2025-02-12 22:14:39","2025-02-12 22:14:39","","3898-3907","","","","","","","","","","","","","","","","","","","","Citation Key: 9706860 ISSN: 2642-9381","","/Users/pcuellar/Zotero/storage/C435S9JH/Agarwal et al. - 2022 - Does data repair lead to fair models Curating contextually fair data to reduce model bias.pdf","","Accountability; Data models; Prediction algorithms; Training; Object detection; Predictive models; Fairness; Explainable AI; Maintenance engineering; Neural networks; Privacy and Ethics in Vision","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLXDUN3B","conferencePaper","2024","B, Ankita; H, ChienChen; P, Lauren; O, Lydia","Impact of explainable AI on reduction of algorithm bias in facial recognition technologies","2024 systems and information engineering design symposium (SIEDS)","","","10.1109/SIEDS61124.2024.10534745","","Artificial Intelligence (AI) has grown dramatically over the past few decades and has much greater influence today on human performance in every walk of life. AI is great for accelerating our work, but there are also downsides to it. One such downside is the application of Facial Recognition Technologies (FRT) available today, which has adverse consequences like racial discrimination or wrongful judgement by commercial firms and even law enforcement organizations. The landmark ‘Gender Shades’ project [1] in 2018 showed a highly skewed error bar between facial recognition accuracies of subjects who are black females compared to white males using an intersectional comparison approach between facial recognition software by IBM, Microsoft and Face++. This supplemented previous studies which highlighted issues with gender classification or on the whole misidentification issues posed by facial recognition technologies (FRT) [2]. This algorithm bias towards misgendering or misclassification based on skin color could be attributed to a highly unbalanced training datasets that lack diversity as well as a lack of understanding of the black box machine learning (ML) models that are used to build the FRTs. From a Code of Ethics perspective, such algorithms do not conform to the fundamental canons [3] such as prioritizing the safety, and welfare of the public. Furthermore, they cannot be used honorably, responsibly and ethically to enhance the reputation, and usefulness of the companies promoting these technologies because they are unable to provide truthful and objective information regarding facial recognition. A possible remedy could be to curate an exhaustively diverse dataset w.r.t both color and gender. However, the task of building such a dataset would require access to a diverse population, which is not always possible in a multi-racial and multi-ethnic society. We thus propose an exhaustive review-based study of existing work on the introduction of explainable AI [4],[5] (XAI) techniques in such ML models to understand how the model itself learns. This study would explore what constraints to put on the learning method of the model, which can enforce reduction in misclassification errors from gender and skin color thus enforcing various ethical aspects like taking care of algorithm bias, introducing transparency and accountability. It would also explore and underline the overall social impact improved FRTs might have from a code of ethics perspective.","2024-05","2025-02-12 22:14:39","2025-02-12 22:14:39","","85-89","","","","","","","","","","","","","","","","","","","","Citation Key: 10534745 ISSN: 2994-3531","","/Users/pcuellar/Zotero/storage/234KRVVA/B et al. - 2024 - Impact of explainable AI on reduction of algorithm bias in facial recognition technologies.pdf","","Sociology; Ethics; Face recognition; Training; Explainable AI; Oral communication; Color","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ARCMP4G","journalArticle","2024","Alderman, Joseph E; Charalambides, Maria; Sachdeva, Gagandeep; Laws, Elinor; Palmer, Joanne; Lee, Elsa; Menon, Vaishnavi; Malik, Qasim; Vadera, Sonam; Calvert, Melanie; Ghassemi, Marzyeh; McCradden, Melissa D; Ordish, Johan; Mateen, Bilal; Summers, Charlotte; Gath, Jacqui; Matin, Rubeta N; Denniston, Alastair K; Liu, Xiaoxuan","Revealing transparency gaps in publicly available COVID-19 datasets used for medical artificial intelligence development—a systematic review","The Lancet Digital Health","","2589-7500","https://doi.org/10.1016/S2589-7500(24)00146-8","https://www.sciencedirect.com/science/article/pii/S2589750024001468","Summary During the COVID-19 pandemic, artificial intelligence (AI) models were created to address health-care resource constraints. Previous research shows that health-care datasets often have limitations, leading to biased AI technologies. This systematic review assessed datasets used for AI development during the pandemic, identifying several deficiencies. Datasets were identified by screening articles from MEDLINE and using Google Dataset Search. 192 datasets were analysed for metadata completeness, composition, data accessibility, and ethical considerations. Findings revealed substantial gaps: only 48","2024","2025-02-12 22:15:54","2025-02-12 22:15:54","","e827-e847","","11","6","","","","","","","","","","","","","","","","","Citation Key: ALDERMAN2024e827","","/Users/pcuellar/Zotero/storage/KZAFXSP7/Alderman et al. - 2024 - Revealing transparency gaps in publicly available COVID-19 datasets used for medical artificial inte.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7RA9VP5","journalArticle","2024","Alencar, Victor; Kohwalter, Troy; Braganholo, Vanessa; da Silva, José Ricardo; Murta, Leonardo","Prov-Dominoes: An approach for knowledge discovery from provenance data","Expert Systems with Applications","","0957-4174","https://doi.org/10.1016/j.eswa.2023.123030","https://www.sciencedirect.com/science/article/pii/S0957417423035327","Provenance has become increasingly relevant to understanding, auditing, and reproducing computational tasks. The provenance analysis processes can often be overwhelming to the user due to the large volume of data, the multiple relationships among data, and the implicit information buried in the data. Existing provenance analysis tools use either visual exploration (which is overwhelming for large provenance graphs) or do not support the exploration of implicit provenance data, such as the inferences of the PROV Data Model Constraints. To fill in this gap, we introduce Prov-Dominoes, a tool designed to interactively enable knowledge discovery on provenance data. Prov-Dominoes promotes the provenance relationships among entities, activities, and agents into first-class elements represented by domino tiles. It allows users to combine and compose such domino tiles visually and interactively, using GPU. The benefits of Prov-Dominoes are three-fold: first, it uses matrices to display provenance data, which is more compact than graphs; second, it allows users to easily explore implicit information; third, it is capable of efficiently processing large datasets using GPUs. We evaluated Prov-Dominoes over distinct case studies, allowing the observation of Prov-Dominoes in action. We also evaluated the performance of sequential combinations executed in Prov-Dominoes when dealing with provenance data with thousands of relations, contrasting their executions in GPU and CPU. The results showed that, for a large dataset, GPU was more than a hundred times faster than CPU.","2024","2025-02-12 22:15:54","2025-02-12 22:15:54","","123030","","","245","","","","","","","","","","","","","","","","","Citation Key: ALENCAR2024123030","","","","Data analysis; Provenance; Knowledge discovery; Gpu computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54QTZEMZ","journalArticle","2025","Alderman, Joseph E; Palmer, Joanne; Laws, Elinor; McCradden, Melissa D; Ordish, Johan; Ghassemi, Marzyeh; Pfohl, Stephen R; Rostamzadeh, Negar; Cole-Lewis, Heather; Glocker, Ben; Calvert, Melanie; Pollard, Tom J; Gill, Jaspret; Gath, Jacqui; Adebajo, Adewale; Beng, Jude; Leung, Cassandra H; Kuku, Stephanie; Farmer, Lesley-Anne; Matin, Rubeta N; Mateen, Bilal A; McKay, Francis; Heller, Katherine; Karthikesalingam, Alan; Treanor, Darren; Mackintosh, Maxine; Oakden-Rayner, Lauren; Pearson, Russell; Manrai, Arjun K; Myles, Puja; Kumuthini, Judit; Kapacee, Zoher; Sebire, Neil J; Nazer, Lama H; Seah, Jarrel; Akbari, Ashley; Berman, Lew; Gichoya, Judy W; Righetto, Lorenzo; Samuel, Diana; Wasswa, William; Charalambides, Maria; Arora, Anmol; Pujari, Sameer; Summers, Charlotte; Sapey, Elizabeth; Wilkinson, Sharon; Thakker, Vishal; Denniston, Alastair; Liu, Xiaoxuan","Tackling algorithmic bias and promoting transparency in health datasets: the STANDING Together consensus recommendations","The Lancet Digital Health","","2589-7500","https://doi.org/10.1016/S2589-7500(24)00224-3","https://www.sciencedirect.com/science/article/pii/S2589750024002243","Summary Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective.","2025","2025-02-12 22:15:55","2025-02-12 22:15:55","","e64-e88","","1","7","","","","","","","","","","","","","","","","","Citation Key: ALDERMAN2025e64","","/Users/pcuellar/Zotero/storage/Z5CMHN2J/Alderman et al. - 2025 - Tackling algorithmic bias and promoting transparency in health datasets the STANDING Together conse.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AH6W4229","preprint","2024","Ahmad, Raia Abu; D'Souza, Jennifer; Zloch, Matthäus; Otto, Wolfgang; Rehm, Georg; Oelen, Allard; Dietze, Stefan; Auer, Sören","Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open Research Knowledge Graph","","","","10.48550/arXiv.2404.08443","http://arxiv.org/abs/2404.08443","Search engines these days can serve datasets as search results. Datasets get picked up by search technologies based on structured descriptions on their official web pages, informed by metadata ontologies such as the Dataset content type of schema.org. Despite this promotion of the content type dataset as a first-class citizen of search results, a vast proportion of datasets, particularly research datasets, still need to be made discoverable and, therefore, largely remain unused. This is due to the sheer volume of datasets released every day and the inability of metadata to reflect a dataset's content and context accurately. This work seeks to improve this situation for a specific class of datasets, namely research datasets, which are the result of research endeavors and are accompanied by a scholarly publication. We propose the ORKG-Dataset content type, a specialized branch of the Open Research Knowledge Graoh (ORKG) platform, which provides descriptive information and a semantic model for research datasets, integrating them with their accompanying scholarly publications. This work aims to establish a standardized framework for recording and reporting research datasets within the ORKG-Dataset content type. This, in turn, increases research dataset transparency on the web for their improved discoverability and applied use. In this paper, we present a proposal -- the minimum FAIR, comparable, semantic description of research datasets in terms of salient properties of their supporting publication. We design a specific application of the ORKG-Dataset semantic model based on 40 diverse research datasets on scientific information extraction.","2024-04-12","2025-02-13 14:47:57","2025-02-13 14:47:57","2025-02-13 14:47:57","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2404.08443 [cs]","","","","","Computer Science - Digital Libraries; Computer Science - Information Retrieval","","","","","","","","","","","","","","","","","","","arXiv:2404.08443","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZNMHHKK","preprint","2023","Bianchi, Andrea; d'Aloisio, Giordano; Marzi, Francesca; Marco, Antinisca Di","A Decision Tree to Shepherd Scientists through Data Retrievability","","","","10.48550/arXiv.2304.05767","http://arxiv.org/abs/2304.05767","Reproducibility is a crucial aspect of scientific research that involves the ability to independently replicate experimental results by analysing the same data or repeating the same experiment. Over the years, many works have been proposed to make the results of the experiments actually reproducible. However, very few address the importance of data reproducibility, defined as the ability of independent researchers to retain the same dataset used as input for experimentation. Properly addressing the problem of data reproducibility is crucial because often just providing a link to the data is not enough to make the results reproducible. In fact, also proper metadata (e.g., preprocessing instruction) must be provided to make a dataset fully reproducible. In this work, our aim is to fill this gap by proposing a decision tree to sheperd researchers through the reproducibility of their datasets. In particular, this decision tree guides researchers through identifying if the dataset is actually reproducible and if additional metadata (i.e., additional resources needed to reproduce the data) must also be provided. This decision tree will be the foundation of a future application that will automate the data reproduction process by automatically providing the necessary metadata based on the particular context (e.g., data availability, data preprocessing, and so on). It is worth noting that, in this paper, we detail the steps to make a dataset retrievable, while we will detail other crucial aspects for reproducibility (e.g., dataset documentation) in future works.","2023-04-12","2025-02-13 14:48:22","2025-02-13 14:48:22","2025-02-13 14:48:22","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.05767 [cs]","","/Users/pcuellar/Zotero/storage/H3JI9VYW/Bianchi et al. - 2023 - A Decision Tree to Shepherd Scientists through Data Retrievability.pdf","","","Computer Science - Digital Libraries","","","","","","","","","","","","","","","","","","","arXiv:2304.05767","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPPG5MPW","preprint","2021","Bandy, Jack; Vincent, Nicholas","Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus","","","","10.48550/arXiv.2105.05241","http://arxiv.org/abs/2105.05241","Recent literature has underscored the importance of dataset documentation work for machine learning, and part of this work involves addressing ""documentation debt"" for datasets that have been used widely but documented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text dataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, even though little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a preliminary datasheet that provides key context and information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence that (1) BookCorpus likely violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, including problematic content, potential skews in religious representation, and lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus adds to growing literature that urges more careful and systematic documentation for machine learning datasets.","2021-05-11","2025-02-13 14:48:46","2025-02-13 14:48:46","2025-02-13 14:48:46","","","","","","","Addressing ""Documentation Debt"" in Machine Learning Research","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.05241 [cs]","","","","","Computer Science - Computers and Society; Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2105.05241","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUTJGY8F","journalArticle","2025","Bhardwaj, Eshta; Gujral, Harshit; Wu, Siyi; Zogheib, Ciara; Maharaj, Tegan; Becker, Christoph","The State of Data Curation at NeurIPS: An Assessment of Dataset Development Practices in the Datasets and Benchmarks Track","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2024/hash/605bbd006beee7e0589a51d6a50dcae1-Abstract-Datasets_and_Benchmarks_Track.html","","2025-01-31","2025-02-24 20:19:17","2025-02-24 20:19:17","2025-02-24 20:19:00","53626-53648","","","37","","","The State of Data Curation at NeurIPS","","","","","","","en","","","","","proceedings.neurips.cc","","","","/Users/pcuellar/Zotero/storage/4J72NJZV/Bhardwaj et al. - 2025 - The State of Data Curation at NeurIPS An Assessment of Dataset Development Practices in the Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BDM3GZAE","conferencePaper","2024","Abouelkhair, Amr; Majdi, Kiarash; Limam, Noura; Salahuddin, Mohammad A.; Boutaba, Raouf","5GProvGen: 5G Provenance Dataset Generation Framework","2024 20th International Conference on Network and Service Management (CNSM)","","","10.23919/CNSM62983.2024.10814296","https://ieeexplore.ieee.org/document/10814296","The softwarization and virtualization of the fifth-generation (5G) cellular networks bring about increased flexibility and faster deployment of new services. However, these advancements also introduce new vulnerabilities and unprecedented attack surfaces. The cloud-native nature of 5G networks mandates detecting and protecting against threats and intrusions in the cloud systems. Additionally, the evolving cyber-threat landscape and the growing reliance on cellular networks for mission-critical tasks reinforce the need for robust security systems, which should be capable of detecting stealthy and zero-day attacksRecent developments in Provenance-based Intrusion Detection Systems (PIDS) address these requirements. These host-based systems aim to analyze provenance graphs derived from system calls to uncover any deviation from the expected benign behaviour of the host. Provenance graphs are structured as holistic representations of the dependencies and causal relationships between digital objects, and hence they fit well in the Service-based Architecture (SBA) of 5G networks. However, deploying PIDS requires substantial datasets of provenance graphs collected from the relevant hosts. In this work, we propose a framework to generate provenance graphs datasets for a 5G core network. We provide an example dataset and evaluate the state-of-the-art PIDS in protecting a 5G network core from various threats.","2024-10","2025-02-27 21:30:39","2025-02-27 21:30:39","2025-02-27 21:30:39","1-9","","","","","","5GProvGen","","","","","","","","","","","","IEEE Xplore","","ISSN: 2165-963X","","/Users/pcuellar/Zotero/storage/8Q2ZYTSE/Abouelkhair et al. - 2024 - 5GProvGen 5G Provenance Dataset Generation Framework.pdf","","","5G; 5G mobile communication; Cellular networks; Cloud computing; dataset; GNN; Intrusion detection; Mission critical systems; provenance; Security; service-based infrastructure; Source coding; Virtualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 20th International Conference on Network and Service Management (CNSM)","","","","","","","","","","","","","","",""
"II3XAA2D","conferencePaper","2021","Afzal, Shazia; Rajmohan, C; Kesarwani, Manish; Mehta, Sameep; Patel, Hima","Data Readiness Report","2021 IEEE International Conference on Smart Data Services (SMDS)","","","10.1109/SMDS53860.2021.00016","https://ieeexplore.ieee.org/abstract/document/9592479","Data exploration and quality analysis is an important yet tedious process in the AI pipeline. Current data cleaning and data readiness assessment practices for machine learning tasks are mostly conducted in an arbitrary manner which limits their reuse and often results in loss of productivity. We introduce the concept of a Data Readiness Report as accompanying documentation to a dataset that allows data consumers to get detailed insights into the quality of data. Data characteristics and challenges on various quality dimensions are identified and documented, keeping in mind the principles of transparency and explainability. The Data Readiness Report also serves as a record of all data assessment operations, including applied transformations. This provides a detailed lineage for data governance and management. In effect, the report captures and documents the actions taken by various personas in a data readiness and assessment workflow. Over time this becomes a repository of best practices and can potentially drive a recommendation system for building automated data readiness workflows on the lines of AutoML [1]. The data readiness report could serve as a valuable asset for organizing and operationalizing data in a Data-as-a-service model as it augments the trust and reliability of the datasets. We anticipate that together with the Datasheets [2], Dataset Nutrition Label [3], FactSheets [4] and Model Cards [5], the Data Readiness Report completes the AI documentation pipeline and increases trust and re-useability of data.","2021-09","2025-02-27 21:31:13","2025-02-27 21:31:13","2025-02-27 21:31:13","42-51","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/pcuellar/Zotero/storage/D9L4S8YH/Afzal et al. - 2021 - Data Readiness Report.pdf","","","Conferences; data assurance and trust; data documentation and governance; Data integrity; Data models; data quality; Documentation; Machine learning; machine learning datasets; Pipelines; Productivity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE International Conference on Smart Data Services (SMDS)","","","","","","","","","","","","","","",""
"J4N2TPJ3","journalArticle","2019","Arslan, Ruben C.","How to Automatically Document Data With the codebook Package to Facilitate Data Reuse","Advances in Methods and Practices in Psychological Science","","2515-2459","10.1177/2515245919838783","https://doi.org/10.1177/2515245919838783","Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once—by their creators—and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.","2019-06-01","2025-03-04 17:13:38","2025-03-04 17:13:38","2025-03-04 17:13:38","169-187","","2","2","","","","","","","","","","en","","","","","SAGE Journals","","Publisher: SAGE Publications Inc","","/Users/pcuellar/Zotero/storage/V66X6WYB/Arslan - 2019 - How to Automatically Document Data With the codebook Package to Facilitate Data Reuse.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""